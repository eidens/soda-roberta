version: '2.3'

services:
  nlp:
    build:
      context: ./src
      dockerfile: '../Dockerfile'
      # args: # abandoning this for now, transfomers needs to be abel to mkdir /.cache
      #   - user_id=${USER_ID}
      #   - group_id=${GROUP_ID}
    image: sdbert
    depends_on:
    - tensorboard
    - celery
    - flower
    runtime: nvidia
    volumes: &volumes
    - ./src:/app
    - ./data:/data
    - ./tokenizer:${TOKENIZER_PATH}
    - ./lm_models:${LM_MODEL_PATH}
    - ./tokcl_models:${TOKCL_MODEL_PATH}
    - ./cache:${CACHE}
    - ./runs:${RUNS_DIR}
    env_file:
    - ./.env
    working_dir: /app
    entrypoint: ["/bin/bash"]

  tensorboard:
    image: sdbert
    runtime: nvidia
    ports:
    - 6007:6007
    volumes:
    - ./runs:/runs
    working_dir: /app
    command: tensorboard --logdir /runs --port 6007 --bind_all

  celery:
    image: sdbert
    depends_on:
    - rabbitmq
    env_file:
    - ./.env
    volumes: *volumes
    working_dir: /app
    command: celery --app=lm worker --loglevel=info

  flower:
    image: sdbert
    depends_on:
    - celery
    volumes: *volumes
    working_dir: /app
    ports:
      - "5555:5555"
    command: flower --app=lm --port=5555 --broker=rabbitmq

  rabbitmq:
    image: rabbitmq:3-management
    ports:
      # The standard AMQP protocol port
      - '5672:5672'
      # HTTP management UI at http://localhost:15672/
      - '15672:15672'
