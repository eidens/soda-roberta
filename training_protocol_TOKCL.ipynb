{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cacc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a2dd817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Config(max_length=512, truncation=True, min_char_length=120, celery_batch_size=1000, from_pretrained='roberta-base', model_type='Autoencoder', nlp=<spacy.lang.en.English object at 0x7fa10a4ae130>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from smtag.config import config\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f0ba2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import __version__\n",
    "__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9464bb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "! more .env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7955b03",
   "metadata": {},
   "source": [
    "## Extracting examples for TOKCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800dbf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.extract import ExtractorXML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9b173",
   "metadata": {},
   "source": [
    "#### Dataset with individual panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24cd3c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_examples = \"/data/text/sd_test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1713e35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr /data/text/sd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1fbdff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor_tokcl = ExtractorXML(\n",
    "    \"/data/xml/191012\",\n",
    "    destination_dir=xml_examples,\n",
    "    sentence_level=False,\n",
    "    xpath=\".//sd-panel\",\n",
    "    keep_xml=True,\n",
    "    inclusion_probability=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc81025e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "extractor_tokcl.extract_from_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4396a",
   "metadata": {},
   "source": [
    "same via CLI:\n",
    "\n",
    "```bash\n",
    "python -m smtag.cli.prepro.extract /data/xml/191012/ /data/text/sd_test --xpath \".//sd-panel\" --sentence_level --keep_xml --inclusion_probability 1.0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc9250",
   "metadata": {},
   "source": [
    "#### Dataset with full figures (used for panelization training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c175a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "xml_figure_examples = \"/data/text/sd_test_figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7c6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr \"/data/text/sd_test_figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c1a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_tokcl_2 = ExtractorXML(\n",
    "    \"/data/xml/191012\",\n",
    "    destination_dir=xml_figure_examples,\n",
    "    sentence_level=False,\n",
    "    xpath=\".//caption\",\n",
    "    keep_xml=True,\n",
    "    inclusion_probability=1.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e127f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_tokcl_2.extract_from_corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2717053",
   "metadata": {},
   "source": [
    "## Preparing tokenized dataset for TOKCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6eb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.dataprep import PreparatorTOKCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdef1b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.xml2labels import SourceDataCodes as sd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf063f4",
   "metadata": {},
   "source": [
    "#### Tokenize panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe30648",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr /data/json/sd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3baafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples = \"/data/json/sd_test\"\n",
    "code_maps: code_maps = [\n",
    "    sd.ENTITY_TYPES,\n",
    "    sd.GENEPROD_ROLES,\n",
    "    sd.SMALL_MOL_ROLES,\n",
    "    sd.BORING,\n",
    "    sd.PANELIZATION\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb11e0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_tokcl = PreparatorTOKCL(\n",
    "    xml_examples,\n",
    "    tokenized_examples,\n",
    "    code_maps,\n",
    "    max_length=config.max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5d173",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_tokcl.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e9c5d3",
   "metadata": {},
   "source": [
    "same vie CLI:\n",
    "    \n",
    "```bash\n",
    "python -m smtag.cli.tokcl.dataprep /data/text/sd_test /data/json/sd_test\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86159135",
   "metadata": {},
   "source": [
    "#### Tokenize figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0dc3d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_figures = \"/data/json/sd_test_figs\"\n",
    "code_maps: code_maps = [\n",
    "    sd.PANELIZATION\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4154b89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr /data/json/sd_test_figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838093e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_tokcl_2 = PreparatorTOKCL(\n",
    "    xml_figure_examples,\n",
    "    tokenized_figures,\n",
    "    code_maps,\n",
    "    max_length=config.max_length\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25685c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prep_tokcl_2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c47466",
   "metadata": {},
   "source": [
    "## Train model for TOKCL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "011ffcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.config import config\n",
    "from smtag.train.train_tokcl import (\n",
    "    train as train_tokcl,\n",
    "    TrainingArgumentsTOKCL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a39776ac",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArgumentsTOKCL(\n",
    "    logging_steps=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9065ceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_examples = \"/data/json/sd_test\"\n",
    "tokenized_figures = \"/data/json/sd_test_figs\"\n",
    "no_cache = False\n",
    "loader_path = \"./smtag/loader/loader_tokcl.py\"\n",
    "tokenizer = config.tokenizer\n",
    "model_type = \"Autoencoder\"\n",
    "from_pretrained = \"EMBO/bio-lm\"  # \"roberta-base\" # specialized model from huggingface.co/embo #  \"roberta-base\" # general lm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7adafa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr /runs/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0757689",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -fr /tokcl_models/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6c88c9",
   "metadata": {},
   "source": [
    "### Train NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3613e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args.overwrite_output_dir=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f555c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArgumentsTOKCL(output_dir='/tokcl_models', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=True, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir='/tokcl_models/runs/Feb16_22-41-22_e6255381d0ac', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=50, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=5, save_on_each_node=False, no_cuda=False, seed=42, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='/tokcl_models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, gradient_checkpointing=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', masking_probability=0.5, replacement_probability=0.0, select_labels=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs=1\n",
    "training_args.prediction_loss_only=True\n",
    "training_args.masking_probability=0.5\n",
    "training_args.replacement_probability=.0\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1be0f242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/datasets/load.py:1650: FutureWarning: 'script_version' was renamed to 'revision' in version 1.13 and will be removed in 1.15.\n",
      "  warnings.warn(\n",
      "WARNING:Using custom data configuration NER-e90bf469e54b0531\n",
      "WARNING:Reusing dataset source_data_nlp (/cache/source_data_nlp/NER-e90bf469e54b0531/0.0.1/be20c29bc25cb738f99d32581855842b36617d57e5a36ca1d6b98499db149e83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /tokcl_models/NER.\n",
      "tokenizer vocab size: 50265\n",
      "\n",
      "Loading and tokenizing datasets found in /data/json/sd_test.\n",
      "using ./smtag/loader/loader_tokcl.py as dataset loader.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f85af3f73bf4bf4a80460ce7bed22b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 15859 examples.\n",
      "Evaluating on 4869 examples.\n",
      "\n",
      "Training on 15 features:\n",
      "O, I-SMALL_MOLECULE, B-SMALL_MOLECULE, I-GENEPROD, B-GENEPROD, I-SUBCELLULAR, B-SUBCELLULAR, I-CELL, B-CELL, I-TISSUE, B-TISSUE, I-ORGANISM, B-ORGANISM, I-EXP_ASSAY, B-EXP_ASSAY\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EMBO/bio-lm were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at EMBO/bio-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training arguments for model type Autoencoder:\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "TrainingArgumentsTOKCL(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/runs/tokcl-NER-2022-02-16T22-41-31.450449,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=50,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "masking_probability=0.5,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "output_dir=/tokcl_models/NER,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=False,\n",
      "replacement_probability=0.0,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tokcl_models,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "select_labels=False,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 15859\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 03:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.507600</td>\n",
       "      <td>0.189321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.146227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.134621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.122400</td>\n",
       "      <td>0.129407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A, B, C. Cell lysates of\u001b[1m\u001b[4m\u001b[38;5;8m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;7mK\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;7m293\u001b[0m cells co-transfected with\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4mF\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m200\u001b[0m (A) and either empty vector control,\u001b[1m\u001b[4m\u001b[38;5;4m My\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mc\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[38;5;3mS\u001b[0m or\u001b[1m\u001b[4m\u001b[38;5;4m My\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4mC\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3morf\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[38;5;3mL\u001b[0m were subjected to\u001b[1m\u001b[4m\u001b[38;5;14m immun\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mitation\u001b[0m with anti-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m (A and B) antibodies. Immune pellets were probed for\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4mC\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m72\u001b[0m (A and B),\u001b[1m\u001b[4m\u001b[38;5;4m FL\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4mF\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mIP\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m200\u001b[0m (A) on\u001b[1m\u001b[4m\u001b[38;5;14m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mobl\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mots\u001b[0m. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mO\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mKO_\u001b[1m\u001b[38;5;4mFH\u001b[0m\u001b[1m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mGO\u001b[0m\u001b[1m\u001b[38;5;7m<mask>\u001b[0m and\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mKO_\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m cell extracts (n=3 experimental replicates) were\u001b[1m\u001b[4m\u001b[38;5;14m immun\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mrec\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mitated\u001b[0m using an anti-\u001b[1m\u001b[4m\u001b[38;5;4mTER\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m antibody or IgG as mock IP.\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m and\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mOTA\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mIR\u001b[0m (as a negative control) abundance was assessed by\u001b[1m\u001b[4m\u001b[38;5;14m RT\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mq\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mPC\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mR\u001b[0m.\u001b[1m\u001b[4m\u001b[38;5;4m H\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mPR\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mT\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m was used for normalization and enrichment in\u001b[1m\u001b[4m\u001b[38;5;4m T\u001b[0m\u001b[1m\u001b[38;5;7m<mask>\u001b[0m\u001b[1m\u001b[38;5;7m-\u001b[0m\u001b[1m\u001b[38;5;7m<mask>\u001b[0m as compared to IgG\u001b[1m\u001b[4m\u001b[38;5;14m<mask>\u001b[0m was plotted. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A\u001b[1m\u001b[4m\u001b[38;5;8m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;7mTS\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;7m-\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;7m2\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;7m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;7m<mask>\u001b[0m cells were incubated with\u001b[1m\u001b[4m\u001b[38;5;8m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;7m21\u001b[0m.221-\u001b[1m\u001b[4m\u001b[38;5;4mC\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m4\u001b[0m or\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mw\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m7\u001b[0m (221-\u001b[1m\u001b[4m\u001b[38;5;4mC\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m4\u001b[0m or\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m) cells for 5 min at 370C. The cells were lysed, and\u001b[1m\u001b[4m\u001b[38;5;14m immun\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mop\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mrec\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m (IP) of\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mP\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m were\u001b[1m\u001b[4m\u001b[38;5;14m immun\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mobl\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13motted\u001b[0m (IB) with anti-p\u001b[1m\u001b[38;5;3mTy\u001b[0m (top panel), anti-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m-\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mact\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3min\u001b[0m (middle panel) or anti-\u001b[1m\u001b[4m\u001b[38;5;4mSH\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m (bottom panel) antibodies. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A Control animals (+/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mIR/+), n= 19, and those with\u001b[1m\u001b[4m\u001b[38;5;10m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9miv\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m gland\u001b[0m-specific knockdown of\u001b[1m\u001b[4m\u001b[38;5;4m sec\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m5\u001b[0m (\u001b[1m\u001b[4m\u001b[38;5;4mf\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mIR/+), n=20, were analyzed by\u001b[1m\u001b[4m\u001b[38;5;14m hist\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m for the presence of\u001b[1m\u001b[4m\u001b[38;5;10m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9miv\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m gland\u001b[0m material (red dotted circle) 24 hours after puparium formation.B Quantification of data from (A). Data are represented as means. Statistical significance was determined using a Chi-square test.C Control animals (+/w; UAS-\u001b[1m\u001b[38;5;4msec\u001b[0m\u001b[1m\u001b[38;5;3m15\u001b[0mIR/+), n= 20, and those with\u001b[1m\u001b[4m\u001b[38;5;10m sal\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m gland\u001b[0m-specific expression\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m (\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mkh\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mIR/+), n= 20, were analyzed by\u001b[1m\u001b[4m\u001b[38;5;14m hist\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mology\u001b[0m for the presence of\u001b[1m\u001b[4m\u001b[38;5;10m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9mary\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m material (red dotted circle) 24 hours after puparium formation.D Quantification of data from (C). Data are represented as means. Statistical significance was determined using a Chi-square test.E Control animals (+/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4msec\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mIR/+), n= 20, and those with\u001b[1m\u001b[4m\u001b[38;5;10m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m gland\u001b[0m-specific expression\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m (\u001b[1m\u001b[4m\u001b[38;5;4mf\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4mG\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mAL\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m4\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4msec\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mIR/+), n= 20, were analyzed by\u001b[1m\u001b[4m\u001b[38;5;14m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mology\u001b[0m for the presence of\u001b[1m\u001b[4m\u001b[38;5;10m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m material (red dotted circle) 24 hours after puparium formation.F Quantification of data from (E). Data are represented as means. Statistical significance was determined using a Chi-square test.G Control animals (+/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4msec\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mIR/+), n= 20, and those with\u001b[1m\u001b[4m\u001b[38;5;10m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m-specific knockdown of\u001b[1m\u001b[4m\u001b[38;5;4m sec\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m8\u001b[0m (\u001b[1m\u001b[4m\u001b[38;5;4mf\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m8\u001b[0mIR/+), n=19, were analyzed by\u001b[1m\u001b[4m\u001b[38;5;14m hist\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13m<mask>\u001b[0m for the presence of\u001b[1m\u001b[4m\u001b[38;5;10m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m gland\u001b[0m material (red dotted circle) 24 hours after puparium formation.H Quantification of data from (G). Data are represented as means. Statistical significance was determined using a Chi-square test.I Control animals (+/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4mex\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mIR/+), n= 19, and those with\u001b[1m\u001b[4m\u001b[38;5;10m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9miv\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m<mask>\u001b[0m-specific knockdown of\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m84\u001b[0m (\u001b[1m\u001b[4m\u001b[38;5;4mf\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3mkh\u001b[0m-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0mIR/+), n=20, were analyzed by\u001b[1m\u001b[4m\u001b[38;5;14m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;13mology\u001b[0m for the presence of\u001b[1m\u001b[4m\u001b[38;5;10m sal\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9miv\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9mary\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;9m gland\u001b[0m material (red dotted circle) 24 hours after puparium formation.J Quant</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tokcl_models/NER\n",
      "Configuration saved in /tokcl_models/NER/config.json\n",
      "Model weights saved in /tokcl_models/NER/pytorch_model.bin\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2399\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 2399.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          CELL       0.75      0.78      0.76      2148\n",
      "     EXP_ASSAY       0.75      0.73      0.74      3328\n",
      "      GENEPROD       0.85      0.89      0.87      9966\n",
      "      ORGANISM       0.75      0.81      0.78      1742\n",
      "SMALL_MOLECULE       0.75      0.74      0.75      2012\n",
      "   SUBCELLULAR       0.67      0.66      0.67      1319\n",
      "        TISSUE       0.66      0.68      0.67      1261\n",
      "\n",
      "     micro avg       0.79      0.81      0.80     21776\n",
      "     macro avg       0.74      0.76      0.75     21776\n",
      "  weighted avg       0.79      0.81      0.80     21776\n",
      "\n",
      "{'test_loss': 0.12268844991922379, 'test_accuracy_score': 0.9619240175306908, 'test_precision': 0.788031594448659, 'test_recall': 0.8109386480529023, 'test_f1': 0.7993210365508656, 'test_runtime': 20.5154, 'test_samples_per_second': 116.937, 'test_steps_per_second': 1.852}\n"
     ]
    }
   ],
   "source": [
    "train_tokcl(\n",
    "    training_args,\n",
    "    loader_path,\n",
    "    \"NER\",\n",
    "    tokenized_examples,\n",
    "    no_cache,\n",
    "    tokenizer,\n",
    "    model_type,\n",
    "    from_pretrained\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1064788f",
   "metadata": {},
   "source": [
    "#### Train GENEPROD ROLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c4323e6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArgumentsTOKCL(output_dir='/tokcl_models', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=True, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir='/tokcl_models/runs/Feb16_22-41-22_e6255381d0ac', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=50, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=5, save_on_each_node=False, no_cuda=False, seed=42, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='/tokcl_models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, gradient_checkpointing=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', masking_probability=1.0, replacement_probability=0.0, select_labels=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 1.0\n",
    "training_args.prediction_loss_only=True\n",
    "training_args.masking_probability=1.\n",
    "training_args.replacement_probability=.0\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "39ec90dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/tokcl_models'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.output_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dbc93fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Using custom data configuration GENEPROD_ROLES-e90bf469e54b0531\n",
      "WARNING:Reusing dataset source_data_nlp (/cache/source_data_nlp/GENEPROD_ROLES-e90bf469e54b0531/0.0.1/be20c29bc25cb738f99d32581855842b36617d57e5a36ca1d6b98499db149e83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /tokcl_models/GENEPROD_ROLES.\n",
      "tokenizer vocab size: 50265\n",
      "\n",
      "Loading and tokenizing datasets found in /data/json/sd_test.\n",
      "using ./smtag/loader/loader_tokcl.py as dataset loader.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4738760d3148454f9469ef5c2ec353e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 15859 examples.\n",
      "Evaluating on 4869 examples.\n",
      "\n",
      "Training on 5 features:\n",
      "O, I-CONTROLLED_VAR, B-CONTROLLED_VAR, I-MEASURED_VAR, B-MEASURED_VAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EMBO/bio-lm/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/09fed88b4a07fe6baced126e3cdb14f2764c1bc57f62d1026a75b3ffdb3ec5f8.c781727f43e25ac5b298f775b2dd4f32f53c9890a2367bbd99ffdbd856251b85\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBO/bio-lm/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f61ee36e7d211df88e7eb67c937348fbd256ab3a970d6b3c9848181ed0330c27.12bb7cf77356bf912ed8598fb9a17becaa7e7c5406692af7264c45956cde2cf3\n",
      "Some weights of the model checkpoint at EMBO/bio-lm were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at EMBO/bio-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15859\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training arguments for model type Autoencoder:\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "TrainingArgumentsTOKCL(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/runs/tokcl-GENEPROD_ROLES-2022-02-16T22-45-44.712065,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=50,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "masking_probability=1.0,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=/tokcl_models/GENEPROD_ROLES,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=False,\n",
      "replacement_probability=0.0,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tokcl_models,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "select_labels=False,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 03:39, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.171100</td>\n",
       "      <td>0.072223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.066700</td>\n",
       "      <td>0.068704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.061300</td>\n",
       "      <td>0.065169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.064815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A, B, C. Cell lysates of HEK293 cells co-transfected with<mask><mask>-\u001b[1m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m (A) and either empty vector control,<mask><mask>-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mS or<mask><mask>-\u001b[1m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0mL were subjected to immunoprecipitation with anti-\u001b[1m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[38;5;3m<mask>\u001b[0m (A and B) antibodies. Immune pellets were probed for<mask><mask>-\u001b[1m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[38;5;3m<mask>\u001b[0m (A and B),<mask><mask>-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m (A) on immunoblots. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mKO_FH<mask><mask><mask> and\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mKO_<mask><mask> cell extracts (n=3 experimental replicates) were immunoprecipitated using an anti-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m antibody or IgG as mock IP.\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m<mask> and<mask><mask><mask> (as a negative control) abundance was assessed by RT-qPCR.<mask><mask><mask><mask> was used for normalization and enrichment in\u001b[1m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m-RIP as compared to IgG RIP was plotted. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A YTS-2DL1 cells were incubated with 721.221-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m or\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m (221-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m or\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m) cells for 5 min at 370C. The cells were lysed, and immunoprecipitates (IP) of\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m were immunoblotted (IB) with anti-p<mask> (top panel), anti-\u001b[1m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[38;5;3m<mask>\u001b[0m (middle panel) or anti-\u001b[1m\u001b[4m\u001b[38;5;4m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;3m<mask>\u001b[0m (bottom panel) antibodies. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A Control animals (+/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m<mask>IR/+), n= 19, and those with salivary gland-specific knockdown of\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m (<mask><mask>-\u001b[1m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mIR/+), n=20, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.B Quantification of data from (A). Data are represented as means. Statistical significance was determined using a Chi-square test.C Control animals (+/w; UAS-sec15IR/+), n= 20, and those with salivary gland-specific expression\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m (<mask><mask>-\u001b[1m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mIR/+), n= 20, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.D Quantification of data from (C). Data are represented as means. Statistical significance was determined using a Chi-square test.E Control animals (+/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mIR/+), n= 20, and those with salivary gland-specific expression\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m (<mask><mask>-\u001b[1m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mIR/+), n= 20, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.F Quantification of data from (E). Data are represented as means. Statistical significance was determined using a Chi-square test.G Control animals (+/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mIR/+), n= 20, and those with salivary gland-specific knockdown of\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m (<mask><mask>-\u001b[1m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mIR/+), n=19, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.H Quantification of data from (G). Data are represented as means. Statistical significance was determined using a Chi-square test.I Control animals (+/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mIR/+), n= 19, and those with salivary gland-specific knockdown of\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m (<mask><mask>-\u001b[1m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[38;5;1m<mask>\u001b[0m/w; UAS-\u001b[1m\u001b[4m\u001b[38;5;2m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0m\u001b[1m\u001b[4m\u001b[38;5;1m<mask>\u001b[0mIR/+), n=20, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.J Quant</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tokcl_models/GENEPROD_ROLES\n",
      "Configuration saved in /tokcl_models/GENEPROD_ROLES/config.json\n",
      "Model weights saved in /tokcl_models/GENEPROD_ROLES/pytorch_model.bin\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2399\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 2399.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "CONTROLLED_VAR       0.74      0.81      0.77      2906\n",
      "  MEASURED_VAR       0.80      0.81      0.80      4151\n",
      "\n",
      "     micro avg       0.77      0.81      0.79      7057\n",
      "     macro avg       0.77      0.81      0.79      7057\n",
      "  weighted avg       0.77      0.81      0.79      7057\n",
      "\n",
      "{'test_loss': 0.05420839041471481, 'test_accuracy_score': 0.9786475863015247, 'test_precision': 0.7709459459459459, 'test_recall': 0.8084171744367294, 'test_f1': 0.789237047796915, 'test_runtime': 19.7826, 'test_samples_per_second': 121.268, 'test_steps_per_second': 1.921}\n"
     ]
    }
   ],
   "source": [
    "train_tokcl(\n",
    "    training_args,\n",
    "    loader_path,\n",
    "    \"GENEPROD_ROLES\",\n",
    "    tokenized_examples,\n",
    "    no_cache,\n",
    "    tokenizer,\n",
    "    model_type,\n",
    "    from_pretrained\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f3a6f5",
   "metadata": {},
   "source": [
    "### Train SMALL MOL ROLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95a88724",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArgumentsTOKCL(output_dir='/tokcl_models', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=True, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=1.0, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir='/tokcl_models/runs/Feb16_22-41-22_e6255381d0ac', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=50, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=5, save_on_each_node=False, no_cuda=False, seed=42, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='/tokcl_models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, gradient_checkpointing=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', masking_probability=1.0, replacement_probability=0.0, select_labels=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 1.0\n",
    "training_args.prediction_loss_only=True\n",
    "training_args.masking_probability=1.0\n",
    "training_args.replacement_probability=.0\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eee8b746",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Using custom data configuration SMALL_MOL_ROLES-e90bf469e54b0531\n",
      "WARNING:Reusing dataset source_data_nlp (/cache/source_data_nlp/SMALL_MOL_ROLES-e90bf469e54b0531/0.0.1/be20c29bc25cb738f99d32581855842b36617d57e5a36ca1d6b98499db149e83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /tokcl_models/SMALL_MOL_ROLES.\n",
      "tokenizer vocab size: 50265\n",
      "\n",
      "Loading and tokenizing datasets found in /data/json/sd_test.\n",
      "using ./smtag/loader/loader_tokcl.py as dataset loader.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d4e94b9b9a749b488361e254450da37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 15859 examples.\n",
      "Evaluating on 4869 examples.\n",
      "\n",
      "Training on 5 features:\n",
      "O, I-CONTROLLED_VAR, B-CONTROLLED_VAR, I-MEASURED_VAR, B-MEASURED_VAR\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EMBO/bio-lm/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/09fed88b4a07fe6baced126e3cdb14f2764c1bc57f62d1026a75b3ffdb3ec5f8.c781727f43e25ac5b298f775b2dd4f32f53c9890a2367bbd99ffdbd856251b85\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBO/bio-lm/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f61ee36e7d211df88e7eb67c937348fbd256ab3a970d6b3c9848181ed0330c27.12bb7cf77356bf912ed8598fb9a17becaa7e7c5406692af7264c45956cde2cf3\n",
      "Some weights of the model checkpoint at EMBO/bio-lm were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at EMBO/bio-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 15859\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training arguments for model type Autoencoder:\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "TrainingArgumentsTOKCL(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/runs/tokcl-SMALL_MOL_ROLES-2022-02-16T22-49-48.225981,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=50,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "masking_probability=1.0,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1.0,\n",
      "output_dir=/tokcl_models/SMALL_MOL_ROLES,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=False,\n",
      "replacement_probability=0.0,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tokcl_models,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "select_labels=False,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='248' max='248' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [248/248 03:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.096900</td>\n",
       "      <td>0.018253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.014983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.012900</td>\n",
       "      <td>0.012849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.012811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A, B, C. Cell lysates of HEK293 cells co-transfected with FLAG-FIP200 (A) and either empty vector control, Myc-C9orf72S or Myc-C9orf72L were subjected to immunoprecipitation with anti-Myc (A and B) antibodies. Immune pellets were probed for Myc-C9orf72 (A and B), FLAG-FIP200 (A) on immunoblots. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>AGO2KO_FH AGO2 and AGO2KO_GFP cell extracts (n=3 experimental replicates) were immunoprecipitated using an anti-TERT antibody or IgG as mock IP. TERC and HOTAIR (as a negative control) abundance was assessed by RT-qPCR. HPRT1 was used for normalization and enrichment in TERT-RIP as compared to IgG RIP was plotted. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A YTS-2DL1 cells were incubated with 721.221-Cw4 or Cw7 (221-Cw4 or Cw7) cells for 5 min at 370C. The cells were lysed, and immunoprecipitates (IP) of SHP-1 were immunoblotted (IB) with anti-pTy (top panel), anti-β-actin (middle panel) or anti-SHP-1 (bottom panel) antibodies. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4869\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>A Control animals (+/w; UAS-sec5IR/+), n= 19, and those with salivary gland-specific knockdown of sec5 (fkh-GAL4/w; UAS-sec5IR/+), n=20, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.B Quantification of data from (A). Data are represented as means. Statistical significance was determined using a Chi-square test.C Control animals (+/w; UAS-sec15IR/+), n= 20, and those with salivary gland-specific expression sec15 (fkh-GAL4/w; UAS-sec15IR/+), n= 20, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.D Quantification of data from (C). Data are represented as means. Statistical significance was determined using a Chi-square test.E Control animals (+/w; UAS-sec3IR/+), n= 20, and those with salivary gland-specific expression sec3 (fkh-GAL4/w; UAS-sec3IR/+), n= 20, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.F Quantification of data from (E). Data are represented as means. Statistical significance was determined using a Chi-square test.G Control animals (+/w; UAS-sec8IR/+), n= 20, and those with salivary gland-specific knockdown of sec8 (fkh-GAL4/w; UAS-sec8IR/+), n=19, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.H Quantification of data from (G). Data are represented as means. Statistical significance was determined using a Chi-square test.I Control animals (+/w; UAS-exo84IR/+), n= 19, and those with salivary gland-specific knockdown of exo84 (fkh-GAL4/w; UAS-exo84IR/+), n=20, were analyzed by histology for the presence of salivary gland material (red dotted circle) 24 hours after puparium formation.J Quant</s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tokcl_models/SMALL_MOL_ROLES\n",
      "Configuration saved in /tokcl_models/SMALL_MOL_ROLES/config.json\n",
      "Model weights saved in /tokcl_models/SMALL_MOL_ROLES/pytorch_model.bin\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2399\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 2399.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                \n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "CONTROLLED_VAR       0.75      0.92      0.83      1098\n",
      "  MEASURED_VAR       0.74      0.79      0.76       313\n",
      "\n",
      "     micro avg       0.75      0.89      0.81      1411\n",
      "     macro avg       0.74      0.85      0.79      1411\n",
      "  weighted avg       0.75      0.89      0.81      1411\n",
      "\n",
      "{'test_loss': 0.0130100566893816, 'test_accuracy_score': 0.9945922822990064, 'test_precision': 0.7476190476190476, 'test_recall': 0.890148830616584, 'test_f1': 0.8126819799417665, 'test_runtime': 19.3847, 'test_samples_per_second': 123.757, 'test_steps_per_second': 1.96}\n"
     ]
    }
   ],
   "source": [
    "train_tokcl(\n",
    "    training_args,\n",
    "    loader_path,\n",
    "    \"SMALL_MOL_ROLES\",\n",
    "    tokenized_examples,\n",
    "    no_cache,\n",
    "    tokenizer,\n",
    "    model_type,\n",
    "    from_pretrained\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4265fb44",
   "metadata": {},
   "source": [
    "### Train PANELIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6b63bae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TrainingArgumentsTOKCL(output_dir='/tokcl_models', overwrite_output_dir=False, do_train=False, do_eval=True, do_predict=False, evaluation_strategy=<IntervalStrategy.STEPS: 'steps'>, prediction_loss_only=True, per_device_train_batch_size=16, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=0.0001, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=2.4, max_steps=-1, lr_scheduler_type=<SchedulerType.LINEAR: 'linear'>, warmup_ratio=0.0, warmup_steps=0, log_level=-1, log_level_replica=-1, log_on_each_node=True, logging_dir='/tokcl_models/runs/Feb16_22-41-22_e6255381d0ac', logging_strategy=<IntervalStrategy.STEPS: 'steps'>, logging_first_step=False, logging_steps=20, logging_nan_inf_filter=True, save_strategy=<IntervalStrategy.STEPS: 'steps'>, save_steps=500, save_total_limit=5, save_on_each_node=False, no_cuda=False, seed=42, bf16=False, fp16=False, fp16_opt_level='O1', half_precision_backend='auto', bf16_full_eval=False, fp16_full_eval=False, tf32=None, local_rank=-1, xpu_backend=None, tpu_num_cores=None, tpu_metrics_debug=False, debug=[], dataloader_drop_last=False, eval_steps=50, dataloader_num_workers=0, past_index=-1, run_name='/tokcl_models', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, ignore_data_skip=False, sharded_ddp=[], deepspeed=None, label_smoothing_factor=0.0, adafactor=False, group_by_length=False, length_column_name='length', report_to=['tensorboard'], ddp_find_unused_parameters=None, ddp_bucket_cap_mb=None, dataloader_pin_memory=True, skip_memory_metrics=True, use_legacy_prediction_loop=False, push_to_hub=False, resume_from_checkpoint=None, hub_model_id=None, hub_strategy=<HubStrategy.EVERY_SAVE: 'every_save'>, hub_token=None, gradient_checkpointing=False, fp16_backend='auto', push_to_hub_model_id=None, push_to_hub_organization=None, push_to_hub_token=None, mp_parameters='', masking_probability=0.0, replacement_probability=0.0, select_labels=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args.num_train_epochs = 2.4\n",
    "training_args.prediction_loss_only=True\n",
    "training_args.masking_probability=.0\n",
    "training_args.replacement_probability=.0\n",
    "training_args.logging_steps=20\n",
    "training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3660eb7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:Using custom data configuration PANELIZATION-0aba30de11d523a2\n",
      "WARNING:Reusing dataset source_data_nlp (/cache/source_data_nlp/PANELIZATION-0aba30de11d523a2/0.0.1/be20c29bc25cb738f99d32581855842b36617d57e5a36ca1d6b98499db149e83)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created /tokcl_models/PANELIZATION.\n",
      "tokenizer vocab size: 50265\n",
      "\n",
      "Loading and tokenizing datasets found in /data/json/sd_test_figs.\n",
      "using ./smtag/loader/loader_tokcl.py as dataset loader.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cafdd31a5cd4d9791f911e6c4957d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with 4445 examples.\n",
      "Evaluating on 1296 examples.\n",
      "\n",
      "Training on 2 features:\n",
      "O, B-PANEL_START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/EMBO/bio-lm/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/09fed88b4a07fe6baced126e3cdb14f2764c1bc57f62d1026a75b3ffdb3ec5f8.c781727f43e25ac5b298f775b2dd4f32f53c9890a2367bbd99ffdbd856251b85\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/EMBO/bio-lm/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/f61ee36e7d211df88e7eb67c937348fbd256ab3a970d6b3c9848181ed0330c27.12bb7cf77356bf912ed8598fb9a17becaa7e7c5406692af7264c45956cde2cf3\n",
      "Some weights of the model checkpoint at EMBO/bio-lm were not used when initializing RobertaForTokenClassification: ['lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at EMBO/bio-lm and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 4445\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training arguments for model type Autoencoder:\n",
      "RobertaConfig {\n",
      "  \"_name_or_path\": \"EMBO/bio-lm\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.15.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "TrainingArgumentsTOKCL(\n",
      "_n_gpu=4,\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "bf16=False,\n",
      "bf16_full_eval=False,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_pin_memory=True,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "do_eval=True,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_steps=50,\n",
      "evaluation_strategy=IntervalStrategy.STEPS,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=False,\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_model_id=None,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=-1,\n",
      "log_level=-1,\n",
      "log_level_replica=-1,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/runs/tokcl-PANELIZATION-2022-02-16T22-53-49.612103,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_type=SchedulerType.LINEAR,\n",
      "masking_probability=0.0,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "no_cuda=False,\n",
      "num_train_epochs=2.4,\n",
      "output_dir=/tokcl_models/PANELIZATION,\n",
      "overwrite_output_dir=False,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=16,\n",
      "per_device_train_batch_size=16,\n",
      "prediction_loss_only=True,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "remove_unused_columns=False,\n",
      "replacement_probability=0.0,\n",
      "report_to=['tensorboard'],\n",
      "resume_from_checkpoint=None,\n",
      "run_name=/tokcl_models,\n",
      "save_on_each_node=False,\n",
      "save_steps=500,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=5,\n",
      "seed=42,\n",
      "select_labels=False,\n",
      "sharded_ddp=[],\n",
      "skip_memory_metrics=True,\n",
      "tf32=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_legacy_prediction_loop=False,\n",
      "warmup_ratio=0.0,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      "xpu_backend=None,\n",
      ")\n",
      "CUDA available: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [168/168 02:02, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.004355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.003175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1296\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>\u001b[1m\u001b[4m\u001b[38;5;1m(\u001b[0ma,b) BJAB lymphoma cells stably expressing mCherry-GFP-LC3 were serially cultured at log phase followed by fluorescence-activated cell sorting for cells with high and low autophagic flux using the ratio of mCherry/GFP (a). The high and low 20% were sorted (a),\u001b[1m\u001b[4m\u001b[38;5;1m (\u001b[0ma,b) BJAB lymphoma cells stably expressing mCherry-GFP-LC3 were serially cultured at log phase followed by fluorescence-activated cell sorting for cells with high and low autophagic flux using the ratio of mCherry/GFP (a). The high and low 20% were sorted (a), re-plated and treated with lysosomal protease inhibitors pepstatin and E-64d for 1��h; lysates were then immunoblotted for the indicated proteins (b).\u001b[1m\u001b[38;5;1m (\u001b[0mc) Densitometry of LC3-II and p62 western blots (normalized to actin and hour 0, mean��±��s.e.m., n��=��3 blots from 2 independent experiments, *P��=��0.051, **P��=��0.0091). (d); autophagic LC3 puncta were assessed by quantitative microscopy (d); autophagic LC3 puncta were assessed by quantitative microscopy (e; punctate area per cell, mean��±��s.e.m., n��=��50 fields, *P��=��0.010, **P��=��0.053).\u001b[1m\u001b[38;5;1m f\u001b[0m) Electron micrographs of HeLa mCherry-GFP-LC3 cells sorted for autophagic flux as in a. Yellow arrows denote autophagosomes; red arrows indicate autolysosomes. (g) Quantification of autophagosomes and autolysomes from electron micrographs (mean��±��s.e.m., n��=��50 fields). Uncropped images of blots are shown in Supplementary Fig. 6. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1296\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>\u001b[1m\u001b[4m\u001b[38;5;1m(\u001b[0mA-B) mRNA for indicated genes in BAT (n=4)\u001b[1m\u001b[4m\u001b[38;5;1m (\u001b[0mc) mRNA for the indicated genes in epididymal white adipose tissue (eWAT) (n=4)\u001b[1m\u001b[4m\u001b[38;5;1m (\u001b[0mD) immunoblots for indicated proteins in BAT from 10‐month (mo)‐old chow diet (RD)‐fed control (Con) and knock out (KO) mice. Arrows depict protein isoforms.\u001b[1m\u001b[4m\u001b[38;5;1m (\u001b[0mE) immunoblots for indicated proteins in eWAT from 10‐month (mo)‐old chow diet (RD)‐fed control (Con) and knock out (KO) mice. Arrows depict protein isoforms.\u001b[1m\u001b[4m\u001b[38;5;1m (\u001b[0mF) Electron micrographs ( × 10,000 magnification) of BAT depicting mitochondria from 4‐mo‐old Con and KO mice. m, mitochondria; LD, lipid droplet; n, nucleus.\u001b[1m\u001b[4m\u001b[38;5;1m (\u001b[0mG) mRNA levels (n=4) in BAT from 10‐mo‐old Con and KO mice\u001b[1m\u001b[4m\u001b[38;5;1m (\u001b[0mH) hematoxylin and eosin (HE) and (I) Sirius Red stains in BAT from 10‐mo‐old Con and KO mice (n=3-4). Average adipocyte and LD size, and LD number in BAT are shown. Scale bar, 50 μm. (J) BAT mRNA levels from 4‐mo‐old RD‐fed Con and KO mice cold‐challenged for 75 min (n=3). Values are mean±s.e. ***P0.001. </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 1296\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<s>\u001b[1m\u001b[4m\u001b[38;5;1m(\u001b[0ma) Primary cortical neurons were transduced with lentivirus expressing sh-RNA for Nrf2 or rat NDP52 (rNDP52), or a scrambled sh-RNA at DIV 1, and were maintained until DIV 6. The levels of tau phosphorylated at Ser262/Ser356 and Ser396/Ser404 were analysed by immunoblotting using 12E8- and PHF1-specific antibodies, respectively. Total tau was detected with a polyclonal tau-specific antibody (Tau). The relative molecular masses (kDa) are indicated to the left of each blot. (b) Bar graph of the relative optical density of phosphorylated tau normalized to actin. Data shown are mean±s.e. of three independent experiments and were analysed using Student's t-test. (*P0.05; ***P0.001) (c,d) Primary cortical neurons were transduced with a control lentivirus (FIGB) or with one expressing humanNDP52 (hNDP52) at DIV 1. To induce autophagy, trehalose (150��mM) was added at DIV 5 and the neurons incubated for 24��h (DIV 6). Primary cortical neurons were fixed with 4% paraformaldehyde, and stained with the 12E8 or PHF1 antibodies. Scale bar, 20��μm. The optical density of tau phosphorylated at Ser262/Ser356 (12E8). (e) and Ser396/Ser404 (PHF1) (f) in the soma of ~30 neurons randomly chosen was analysed with the ImageJ program. Data were analysed using Student's t-test (***P0.001). </s>\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to /tokcl_models/PANELIZATION\n",
      "Configuration saved in /tokcl_models/PANELIZATION/config.json\n",
      "Model weights saved in /tokcl_models/PANELIZATION/pytorch_model.bin\n",
      "***** Running Prediction *****\n",
      "  Num examples = 652\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing on 652.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='11' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/11 00:02]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                                                \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " PANEL_START       0.92      0.97      0.94      1860\n",
      "\n",
      "   micro avg       0.92      0.97      0.94      1860\n",
      "   macro avg       0.92      0.97      0.94      1860\n",
      "weighted avg       0.92      0.97      0.94      1860\n",
      "\n",
      "{'test_loss': 0.002895773621276021, 'test_accuracy_score': 0.9990617418539087, 'test_precision': 0.9226434426229508, 'test_recall': 0.9682795698924731, 'test_f1': 0.9449108079748164, 'test_runtime': 11.4343, 'test_samples_per_second': 57.022, 'test_steps_per_second': 0.962}\n"
     ]
    }
   ],
   "source": [
    "train_tokcl(\n",
    "    training_args,\n",
    "    loader_path,\n",
    "    \"PANELIZATION\",\n",
    "    tokenized_figures,  # Use Figure-level data here!\n",
    "    no_cache,\n",
    "    tokenizer,\n",
    "    model_type,\n",
    "    from_pretrained\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd7ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls /tokcl_models/NER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f29c96",
   "metadata": {},
   "source": [
    "### Alternative via CLI:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d0101d",
   "metadata": {},
   "source": [
    "Useful for testing and debugging from within `tmux` session and `docker-compose exec nlp bash`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2992d00",
   "metadata": {},
   "source": [
    "```bash\n",
    "python -m smtag.cli.tokcl.train \\\n",
    "./smtag/loader/loader_tokcl.py \\\n",
    "PANELIZATION \\\n",
    "--data_dir /data/json/sd_test \\\n",
    "--num_train_epochs=1 \\\n",
    "--logging_steps=50 \\\n",
    "--per_device_train_batch_size=16 \\\n",
    "--per_device_eval_batch_size=16 \\\n",
    "--replacement_probability=0 \\\n",
    "--masking_probability=0 \\\n",
    "--model_type=Autoencoder \\\n",
    "--from_pretrained=\"EMBO/bio-lm\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57edf69c",
   "metadata": {},
   "source": [
    "## Try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f554d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from smtag.pipeline import SmartTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59cae2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "smarttagger = SmartTagger(\n",
    "    tokenizer_source=\"roberta-base\",\n",
    "    panelizer_source=\"/tokcl_models/PANELIZATION\",\n",
    "    ner_source=\"/tokcl_models/NER\",\n",
    "    geneprod_roles_source=\"/tokcl_models/GENEPROD_ROLES\",\n",
    "    small_mol_roles_source=\"/tokcl_models/SMALL_MOL_ROLES\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d131a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"smtag\": [\n",
      "    {\n",
      "      \"panel_group\": [\n",
      "        [\n",
      "          {\n",
      "            \"text\": \"creb1\",\n",
      "            \"type\": \"geneprod\",\n",
      "            \"role\": \"intervention\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"mouse\",\n",
      "            \"type\": \"organism\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"brain\",\n",
      "            \"type\": \"tissue\"\n",
      "          },\n",
      "          {\n",
      "            \"text\": \"aspirin\",\n",
      "            \"type\": \"molecule\",\n",
      "            \"role\": \"intervention\"\n",
      "          }\n",
      "        ]\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "tags = smarttagger(\"This creb1-/- mutant mouse has a strange brain after aspirin treatment.\")\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8f0fc7",
   "metadata": {},
   "source": [
    "With CLI:\n",
    "\n",
    "    python -m smtag.cli.inference.tag --local_model_dir /tokcl_models \"This creb1-/- mutant mouse has a strange brain after aspirin treatment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e6d82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
